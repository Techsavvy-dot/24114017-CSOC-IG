{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":12182365,"sourceType":"datasetVersion","datasetId":7672880}],"dockerImageVersionId":31040,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np \nimport pandas as pd ","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"editable":false,"execution":{"iopub.status.busy":"2025-06-21T17:20:09.408731Z","iopub.execute_input":"2025-06-21T17:20:09.408999Z","iopub.status.idle":"2025-06-21T17:20:09.678531Z","shell.execute_reply.started":"2025-06-21T17:20:09.408978Z","shell.execute_reply":"2025-06-21T17:20:09.677922Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"df = pd.read_csv(r'/kaggle/input/english-to-hindi/hindi_english_parallel.csv')","metadata":{"trusted":true,"editable":false,"execution":{"iopub.status.busy":"2025-06-21T16:02:10.349472Z","iopub.execute_input":"2025-06-21T16:02:10.349819Z","iopub.status.idle":"2025-06-21T16:02:22.192006Z","shell.execute_reply.started":"2025-06-21T16:02:10.349797Z","shell.execute_reply":"2025-06-21T16:02:22.191277Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"(df.isnull().sum())/df.shape[0]","metadata":{"trusted":true,"editable":false,"execution":{"iopub.status.busy":"2025-06-21T16:02:22.195720Z","iopub.execute_input":"2025-06-21T16:02:22.195988Z","iopub.status.idle":"2025-06-21T16:02:22.439948Z","shell.execute_reply.started":"2025-06-21T16:02:22.195967Z","shell.execute_reply":"2025-06-21T16:02:22.439046Z"}},"outputs":[{"execution_count":3,"output_type":"execute_result","data":{"text/plain":"hindi      0.003878\nenglish    0.000562\ndtype: float64"},"metadata":{}}],"execution_count":3},{"cell_type":"code","source":"df = df.dropna()","metadata":{"trusted":true,"editable":false,"execution":{"iopub.status.busy":"2025-06-21T16:02:22.440756Z","iopub.execute_input":"2025-06-21T16:02:22.441003Z","iopub.status.idle":"2025-06-21T16:02:22.792289Z","shell.execute_reply.started":"2025-06-21T16:02:22.440980Z","shell.execute_reply":"2025-06-21T16:02:22.791484Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"x = df['hindi']\ny = df['english']","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-21T16:02:22.793200Z","iopub.execute_input":"2025-06-21T16:02:22.793444Z","iopub.status.idle":"2025-06-21T16:02:22.799578Z","shell.execute_reply.started":"2025-06-21T16:02:22.793401Z","shell.execute_reply":"2025-06-21T16:02:22.798559Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"pip install nltk","metadata":{"trusted":true,"editable":false,"execution":{"iopub.status.busy":"2025-06-21T16:02:22.800487Z","iopub.execute_input":"2025-06-21T16:02:22.800706Z","iopub.status.idle":"2025-06-21T16:02:27.340661Z","shell.execute_reply.started":"2025-06-21T16:02:22.800686Z","shell.execute_reply":"2025-06-21T16:02:27.339663Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (3.9.1)\nRequirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk) (8.1.8)\nRequirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk) (1.5.0)\nRequirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk) (2024.11.6)\nRequirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from nltk) (4.67.1)\nNote: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"import nltk\nnltk.download('punkt')\nfrom nltk.tokenize import word_tokenize","metadata":{"trusted":true,"editable":false,"execution":{"iopub.status.busy":"2025-06-21T16:02:27.343216Z","iopub.execute_input":"2025-06-21T16:02:27.343516Z","iopub.status.idle":"2025-06-21T16:02:28.930695Z","shell.execute_reply.started":"2025-06-21T16:02:27.343488Z","shell.execute_reply":"2025-06-21T16:02:28.929784Z"}},"outputs":[{"name":"stderr","text":"[nltk_data] Downloading package punkt to /usr/share/nltk_data...\n[nltk_data]   Package punkt is already up-to-date!\n","output_type":"stream"}],"execution_count":7},{"cell_type":"code","source":"tokenized_x = x.apply(lambda sentence: word_tokenize(sentence))\ntokenized_y = y.apply(lambda sentence: word_tokenize(sentence))","metadata":{"trusted":true,"editable":false,"execution":{"iopub.status.busy":"2025-06-21T16:02:28.931530Z","iopub.execute_input":"2025-06-21T16:02:28.931904Z","iopub.status.idle":"2025-06-21T16:07:37.841381Z","shell.execute_reply.started":"2025-06-21T16:02:28.931873Z","shell.execute_reply":"2025-06-21T16:07:37.840456Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"!pip install nltk","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-21T16:07:37.843386Z","iopub.execute_input":"2025-06-21T16:07:37.843694Z","iopub.status.idle":"2025-06-21T16:07:41.531067Z","shell.execute_reply.started":"2025-06-21T16:07:37.843672Z","shell.execute_reply":"2025-06-21T16:07:41.529963Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (3.9.1)\nRequirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk) (8.1.8)\nRequirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk) (1.5.0)\nRequirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk) (2024.11.6)\nRequirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from nltk) (4.67.1)\n","output_type":"stream"}],"execution_count":9},{"cell_type":"code","source":"from collections import Counter\n\n# Flatten all token lists into one big list\nall_hi_tokens = [token for sentence in tokenized_x for token in sentence]\nall_en_tokens = [token for sentence in tokenized_y for token in sentence]\n\n# Count frequencies\nhi_counter = Counter(all_hi_tokens)\nen_counter = Counter(all_en_tokens)\n\n# Define special tokens\nspecial_tokens = [\"<pad>\", \"<sos>\", \"<eos>\", \"<unk>\"]\n\n# Build vocab lists starting with special tokens\nhi_vocab_list = special_tokens + sorted(hi_counter)\nen_vocab_list = special_tokens + sorted(en_counter)\n\n# Make word-to-index mappings\nhi_vocab = {word: idx for idx, word in enumerate(hi_vocab_list)}\nen_vocab = {word: idx for idx, word in enumerate(en_vocab_list)}\n\n# Optional: reverse lookups too\nhi_ivocab = {idx: word for word, idx in hi_vocab.items()}\nen_ivocab = {idx: word for word, idx in en_vocab.items()}\n\ndef encode(tokens, vocab):\n    return [vocab.get(\"<sos>\")] + [vocab.get(tok, vocab[\"<unk>\"]) for tok in tokens] + [vocab.get(\"<eos>\")]\n","metadata":{"trusted":true,"editable":false,"execution":{"iopub.status.busy":"2025-06-21T16:07:41.532324Z","iopub.execute_input":"2025-06-21T16:07:41.532622Z","iopub.status.idle":"2025-06-21T16:07:54.681539Z","shell.execute_reply.started":"2025-06-21T16:07:41.532593Z","shell.execute_reply":"2025-06-21T16:07:54.680646Z"}},"outputs":[],"execution_count":10},{"cell_type":"code","source":"import gc\ndel all_hi_tokens,all_en_tokens,hi_counter,en_counter,hi_vocab_list,en_vocab_list\ngc.collect()","metadata":{"trusted":true,"editable":false,"execution":{"iopub.status.busy":"2025-06-21T16:07:54.682568Z","iopub.execute_input":"2025-06-21T16:07:54.683305Z","iopub.status.idle":"2025-06-21T16:07:56.896229Z","shell.execute_reply.started":"2025-06-21T16:07:54.683263Z","shell.execute_reply":"2025-06-21T16:07:56.895466Z"}},"outputs":[{"execution_count":11,"output_type":"execute_result","data":{"text/plain":"0"},"metadata":{}}],"execution_count":11},{"cell_type":"code","source":"x_ids = tokenized_x.apply(lambda tokens: encode(tokens, hi_vocab))\ny_ids = tokenized_y.apply(lambda tokens: encode(tokens, en_vocab))","metadata":{"trusted":true,"editable":false,"execution":{"iopub.status.busy":"2025-06-21T16:07:56.897133Z","iopub.execute_input":"2025-06-21T16:07:56.897401Z","iopub.status.idle":"2025-06-21T16:08:17.839959Z","shell.execute_reply.started":"2025-06-21T16:07:56.897382Z","shell.execute_reply":"2025-06-21T16:08:17.838958Z"}},"outputs":[],"execution_count":12},{"cell_type":"code","source":"del tokenized_x,tokenized_y\ngc.collect()","metadata":{"trusted":true,"editable":false,"execution":{"iopub.status.busy":"2025-06-21T16:08:17.842509Z","iopub.execute_input":"2025-06-21T16:08:17.842752Z","iopub.status.idle":"2025-06-21T16:08:20.091824Z","shell.execute_reply.started":"2025-06-21T16:08:17.842734Z","shell.execute_reply":"2025-06-21T16:08:20.090898Z"}},"outputs":[{"execution_count":13,"output_type":"execute_result","data":{"text/plain":"0"},"metadata":{}}],"execution_count":13},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\n\nclass EncoderGRU(nn.Module):\n    def __init__(self, input_dim, emb_dim, hidden_dim):\n        super().__init__()\n        self.embedding = nn.Embedding(input_dim, emb_dim)\n        self.gru = nn.GRU(emb_dim, hidden_dim)\n\n    def forward(self, x):\n        embedded = self.embedding(x)  # [src_len, batch_size, emb_dim]\n        outputs, hidden = self.gru(embedded)  # [src_len, batch_size, hidden_dim]\n        return outputs, hidden\n    \nclass DecoderGRU(nn.Module):\n    def __init__(self, output_dim, emb_dim, hidden_dim):\n        super().__init__()\n        self.embedding = nn.Embedding(output_dim, emb_dim)\n        self.gru = nn.GRU(emb_dim, hidden_dim)\n        self.fc_out = nn.Linear(hidden_dim, output_dim)\n\n    def forward(self, input, hidden,):\n        input = input.unsqueeze(0)  # [1, batch_size]\n        embedded = self.embedding(input)  # [1, batch_size, emb_dim]\n        output, hidden = self.gru(embedded, hidden)\n        prediction = self.fc_out(output.squeeze(0))  # [batch_size, output_dim]\n        return prediction, hidden\n\nclass Seq2Seq(nn.Module):\n    def __init__(self, encoder, decoder):\n        super().__init__()\n        self.encoder = encoder\n        self.decoder = decoder\n\n    def forward(self, x, y, teacher_forcing_ratio=0.1):\n        batch_size = y.shape[1]\n        y_len = y.shape[0]\n        y_vocab_size = self.decoder.fc_out.out_features\n        outputs = torch.zeros(y_len, batch_size, y_vocab_size)\n\n        encoder_outputs, hidden = self.encoder(x)\n        input = y[0, :]  #token\n\n        for t in range(1, y_len):\n            output, hidden= self.decoder(input, hidden)\n            outputs[t] = output\n            top1 = output.argmax(1)\n            input = y[t] if torch.rand(1).item() < teacher_forcing_ratio else top1\n\n        return outputs","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-21T16:08:20.093049Z","iopub.execute_input":"2025-06-21T16:08:20.093392Z","iopub.status.idle":"2025-06-21T16:08:22.061687Z","shell.execute_reply.started":"2025-06-21T16:08:20.093362Z","shell.execute_reply":"2025-06-21T16:08:22.060840Z"}},"outputs":[],"execution_count":14},{"cell_type":"code","source":"INPUT_DIM = len(hi_vocab)\nOUTPUT_DIM = len(en_vocab)\nEMB_DIM = 256\nHID_DIM = 512\n\nencoder = EncoderGRU(INPUT_DIM, EMB_DIM, HID_DIM)\ndecoder = DecoderGRU(OUTPUT_DIM, EMB_DIM, HID_DIM)\n\nmodel = Seq2Seq(encoder, decoder)\n","metadata":{"trusted":true,"editable":false,"execution":{"iopub.status.busy":"2025-06-21T16:08:22.062592Z","iopub.execute_input":"2025-06-21T16:08:22.063074Z","iopub.status.idle":"2025-06-21T16:08:25.173299Z","shell.execute_reply.started":"2025-06-21T16:08:22.063049Z","shell.execute_reply":"2025-06-21T16:08:25.172483Z"}},"outputs":[],"execution_count":15},{"cell_type":"code","source":"del EncoderGRU,DecoderGRU\ngc.collect()","metadata":{"trusted":true,"editable":false,"execution":{"iopub.status.busy":"2025-06-21T16:08:25.174251Z","iopub.execute_input":"2025-06-21T16:08:25.174543Z","iopub.status.idle":"2025-06-21T16:08:26.212770Z","shell.execute_reply.started":"2025-06-21T16:08:25.174519Z","shell.execute_reply":"2025-06-21T16:08:26.211899Z"}},"outputs":[{"execution_count":16,"output_type":"execute_result","data":{"text/plain":"30"},"metadata":{}}],"execution_count":16},{"cell_type":"code","source":"device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-21T16:08:26.213623Z","iopub.execute_input":"2025-06-21T16:08:26.213854Z","iopub.status.idle":"2025-06-21T16:08:26.233198Z","shell.execute_reply.started":"2025-06-21T16:08:26.213836Z","shell.execute_reply":"2025-06-21T16:08:26.232326Z"}},"outputs":[],"execution_count":17},{"cell_type":"code","source":"device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel = model.to(device)","metadata":{"trusted":true,"editable":false,"execution":{"iopub.status.busy":"2025-06-21T12:09:48.559514Z","iopub.execute_input":"2025-06-21T12:09:48.560588Z","iopub.status.idle":"2025-06-21T12:09:48.575195Z","shell.execute_reply.started":"2025-06-21T12:09:48.560551Z","shell.execute_reply":"2025-06-21T12:09:48.574126Z"}},"outputs":[],"execution_count":59},{"cell_type":"code","source":"del x,y,df\ngc.collect()","metadata":{"trusted":true,"editable":false,"execution":{"iopub.status.busy":"2025-06-21T16:08:26.234213Z","iopub.execute_input":"2025-06-21T16:08:26.234556Z","iopub.status.idle":"2025-06-21T16:08:27.509511Z","shell.execute_reply.started":"2025-06-21T16:08:26.234528Z","shell.execute_reply":"2025-06-21T16:08:27.508694Z"}},"outputs":[{"execution_count":18,"output_type":"execute_result","data":{"text/plain":"0"},"metadata":{}}],"execution_count":18},{"cell_type":"code","source":"import torch.optim as optim\nPAD_IDX = en_vocab[\"<pad>\"]\ncriterion = nn.CrossEntropyLoss(ignore_index=PAD_IDX)\noptimizer = torch.optim.AdamW(filter(lambda p: p.requires_grad, model.parameters()), lr=1e-3)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-21T12:09:51.133969Z","iopub.execute_input":"2025-06-21T12:09:51.134375Z","iopub.status.idle":"2025-06-21T12:09:51.396450Z","shell.execute_reply.started":"2025-06-21T12:09:51.134344Z","shell.execute_reply":"2025-06-21T12:09:51.395599Z"}},"outputs":[],"execution_count":61},{"cell_type":"code","source":"from torch.utils.data import Dataset\n\nclass TranslationDataset(Dataset):\n    def __init__(self, src_data, trg_data):\n        self.src = src_data\n        self.trg = trg_data\n\n    def __len__(self):\n        return len(self.src)\n\n    def __getitem__(self, idx):\n        return self.src[idx], self.trg[idx]\n\nfrom torch.nn.utils.rnn import pad_sequence\n\ndef collate_fn(batch):\n    src_batch, trg_batch = zip(*batch)\n    src_batch = pad_sequence([torch.tensor(s) for s in src_batch], padding_value=hi_vocab[\"<pad>\"])\n    trg_batch = pad_sequence([torch.tensor(t) for t in trg_batch], padding_value=en_vocab[\"<pad>\"])\n    return src_batch, trg_batch\n\nfrom torch.utils.data import DataLoader\n\ndataset = TranslationDataset(x_ids.tolist(), y_ids.tolist())\nloader = DataLoader(dataset, batch_size=16, shuffle=True, collate_fn=collate_fn)\n\n\ndel TranslationDataset,collate_fn,dataset\ngc.collect","metadata":{"trusted":true,"editable":false,"execution":{"iopub.status.busy":"2025-06-21T16:08:27.510733Z","iopub.execute_input":"2025-06-21T16:08:27.510984Z","iopub.status.idle":"2025-06-21T16:08:27.621736Z","shell.execute_reply.started":"2025-06-21T16:08:27.510964Z","shell.execute_reply":"2025-06-21T16:08:27.620958Z"}},"outputs":[{"execution_count":19,"output_type":"execute_result","data":{"text/plain":"<function gc.collect(generation=2)>"},"metadata":{}}],"execution_count":19},{"cell_type":"code","source":"total_batches = len(loader) \nbatch_iterator = iter(loader) \nepoch = 0 \ntotal_loss = 0 \nbatch_iterator = iter(loader) \nfor i in range(total_batches): \n    src_batch, trg_batch = next(batch_iterator) \n    src_batch = src_batch.to(device) \n    trg_batch = trg_batch.to(device) \n    optimizer.zero_grad() \n    output = model(src_batch, trg_batch) \n    output_dim = output.shape[-1] \n    output = output[1:].reshape(-1, output_dim) \n    trg = trg_batch[1:].reshape(-1) \n    loss = criterion(output, trg) \n    loss.backward() \n    optimizer.step() \n    torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n    print(f\"Epoch {epoch+1} | Loss: {loss.item() :.4f}\") \n    if epoch ==10:\n        break\n    epoch+=1\n    del trg,output,output_dim \n    gc.collect() ","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"sentence = \"मैं स्कूल जा रहा हूँ\"\ntokens = word_tokenize(sentence)\n\n# Use Hindi vocab to encode input\nids = [hi_vocab[\"<sos>\"]] + [hi_vocab.get(tok, hi_vocab[\"<unk>\"]) for tok in tokens] + [hi_vocab[\"<eos>\"]]\nsrc_tensor = torch.tensor(ids).unsqueeze(1).to(device)  # [src_len, 1]\n\nwith torch.no_grad():\n    encoder_outputs, hidden = model.encoder(src_tensor)\n\noutput_tokens = []\ninput_token = torch.tensor([en_vocab[\"<sos>\"]]).to(device)  # Decoder starts with English <sos>\n\nfor _ in range(50):  # Max translation length\n    with torch.no_grad():\n        output, hidden= model.decoder(input_token, hidden)\n\n    top1 = output.argmax(1).item()\n    if top1 == en_vocab[\"<eos>\"]:\n        break\n    output_tokens.append(top1)\n    input_token = torch.tensor([top1]).to(device)\n\n# Convert English IDs back to words\nprediction = [en_ivocab.get(tok, \"<unk>\") for tok in output_tokens]\nprint(\" \".join(prediction))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-21T12:09:56.469369Z","iopub.status.idle":"2025-06-21T12:09:56.471250Z","shell.execute_reply.started":"2025-06-21T12:09:56.470353Z","shell.execute_reply":"2025-06-21T12:09:56.470432Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\ntrain_src, test_src, train_trg, test_trg = train_test_split(\n    x_ids.tolist(), y_ids.tolist(), test_size=0.1, random_state=42)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-21T16:08:27.622681Z","iopub.execute_input":"2025-06-21T16:08:27.622995Z","iopub.status.idle":"2025-06-21T16:08:28.815359Z","shell.execute_reply.started":"2025-06-21T16:08:27.622966Z","shell.execute_reply":"2025-06-21T16:08:28.814562Z"}},"outputs":[],"execution_count":20},{"cell_type":"code","source":"def translate(sentence, model, src_vocab, trg_vocab, trg_ivocab, max_len=50, device='cpu'):\n    model.eval()\n\n    # Tokenize input sentence (assuming whitespace tokenization)\n    tokens = word_tokenize(sentence)\n\n    # Convert to source IDs\n    ids = [src_vocab[\"<sos>\"]] + [src_vocab.get(tok, src_vocab[\"<unk>\"]) for tok in tokens] + [src_vocab[\"<eos>\"]]\n    src_tensor = torch.tensor(ids).unsqueeze(1).to(device)  # [src_len, 1]\n\n    with torch.no_grad():\n        encoder_outputs, hidden = model.encoder(src_tensor)\n\n    input_token = torch.tensor([trg_vocab[\"<sos>\"]]).to(device)\n    output_tokens = []\n\n    for _ in range(max_len):\n        with torch.no_grad():\n            output, hidden = model.decoder(input_token, hidden)\n\n        top1 = output.argmax(1).item()\n        if top1 == trg_vocab[\"<eos>\"]:\n            break\n\n        output_tokens.append(top1)\n        input_token = torch.tensor([top1]).to(device)\n\n    prediction = [trg_ivocab.get(tok, \"<unk>\") for tok in output_tokens]\n    return \" \".join(prediction)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-21T12:09:56.479099Z","iopub.status.idle":"2025-06-21T12:09:56.482547Z","shell.execute_reply.started":"2025-06-21T12:09:56.479851Z","shell.execute_reply":"2025-06-21T12:09:56.479877Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"hypotheses = []\nreferences = []\ncount = 0\nfor src_seq, trg_seq in zip(test_src, test_trg):  # token ID lists\n    if count == 5:\n        break\n    print(\"start\")\n    src_sentence = [hi_ivocab.get(tok, \"<unk>\") for tok in src_seq if tok not in [hi_vocab[\"<sos>\"], hi_vocab[\"<eos>\"], hi_vocab[\"<pad>\"]]]\n    ref_sentence = [en_ivocab.get(tok, \"<unk>\") for tok in trg_seq if tok not in [en_vocab[\"<sos>\"], en_vocab[\"<eos>\"], en_vocab[\"<pad>\"]]]\n    print(\"done\")\n    src_text = \" \".join(src_sentence)\n    ref_text = \" \".join(ref_sentence)\n    print(\"done\")\n    prediction = translate(src_text, model, hi_vocab, en_vocab, en_ivocab)  # your existing translate() function\n    print(\"done\")\n    hypotheses.append(prediction)\n    references.append([ref_text])\n    count +=1","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-21T12:09:56.494646Z","iopub.status.idle":"2025-06-21T12:09:56.498844Z","shell.execute_reply.started":"2025-06-21T12:09:56.494951Z","shell.execute_reply":"2025-06-21T12:09:56.494991Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"pip install sacrebleu","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-21T16:08:28.816152Z","iopub.execute_input":"2025-06-21T16:08:28.816397Z","iopub.status.idle":"2025-06-21T16:08:33.024172Z","shell.execute_reply.started":"2025-06-21T16:08:28.816380Z","shell.execute_reply":"2025-06-21T16:08:33.023195Z"}},"outputs":[{"name":"stdout","text":"Collecting sacrebleu\n  Downloading sacrebleu-2.5.1-py3-none-any.whl.metadata (51 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m51.8/51.8 kB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hCollecting portalocker (from sacrebleu)\n  Downloading portalocker-3.2.0-py3-none-any.whl.metadata (8.7 kB)\nRequirement already satisfied: regex in /usr/local/lib/python3.11/dist-packages (from sacrebleu) (2024.11.6)\nRequirement already satisfied: tabulate>=0.8.9 in /usr/local/lib/python3.11/dist-packages (from sacrebleu) (0.9.0)\nRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from sacrebleu) (1.26.4)\nRequirement already satisfied: colorama in /usr/local/lib/python3.11/dist-packages (from sacrebleu) (0.4.6)\nRequirement already satisfied: lxml in /usr/local/lib/python3.11/dist-packages (from sacrebleu) (5.3.1)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->sacrebleu) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->sacrebleu) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->sacrebleu) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->sacrebleu) (2025.1.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->sacrebleu) (2022.1.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->sacrebleu) (2.4.1)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->sacrebleu) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->sacrebleu) (2022.1.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy>=1.17->sacrebleu) (1.3.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy>=1.17->sacrebleu) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy>=1.17->sacrebleu) (2024.2.0)\nDownloading sacrebleu-2.5.1-py3-none-any.whl (104 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m104.1/104.1 kB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading portalocker-3.2.0-py3-none-any.whl (22 kB)\nInstalling collected packages: portalocker, sacrebleu\nSuccessfully installed portalocker-3.2.0 sacrebleu-2.5.1\nNote: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}],"execution_count":21},{"cell_type":"code","source":"from sacrebleu import corpus_bleu\n\nbleu = corpus_bleu(hypotheses, references)\nprint(f\"BLEU Score: {bleu.score:.2f}\")\n","metadata":{"trusted":true,"editable":false,"execution":{"iopub.status.busy":"2025-06-21T12:09:56.523831Z","iopub.status.idle":"2025-06-21T12:09:56.528569Z","shell.execute_reply.started":"2025-06-21T12:09:56.526103Z","shell.execute_reply":"2025-06-21T12:09:56.526434Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"del model\ngc.collect()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-21T16:08:33.025332Z","iopub.execute_input":"2025-06-21T16:08:33.025674Z","iopub.status.idle":"2025-06-21T16:08:34.382673Z","shell.execute_reply.started":"2025-06-21T16:08:33.025643Z","shell.execute_reply":"2025-06-21T16:08:34.381843Z"}},"outputs":[{"execution_count":22,"output_type":"execute_result","data":{"text/plain":"26"},"metadata":{}}],"execution_count":22},{"cell_type":"markdown","source":"# Part 2","metadata":{"editable":false}},{"cell_type":"code","source":"class LuongAttention(nn.Module):\n    def __init__(self, hidden_dim):\n        super().__init__()\n        self.scale = hidden_dim ** 0.5  # optional: scale dot-product\n\n    def forward(self, decoder_hidden, encoder_outputs):\n    \n        attn_scores = torch.sum(decoder_hidden * encoder_outputs, dim=2)  # [src_len, batch]\n\n        #Softmax to get attention weights\n        attn_weights = torch.softmax(attn_scores, dim=0)  # [src_len, batch]\n\n        #Compute context vector\n        context = torch.sum(attn_weights.unsqueeze(2) * encoder_outputs, dim=0)  # [batch, hidden]\n\n        return context, attn_weights \n\nclass AttentionDecoderGRU(nn.Module):\n    def __init__(self, output_dim, emb_dim, hidden_dim, attention):\n        super().__init__()\n        self.output_dim = output_dim\n        self.attention = attention\n\n        self.embedding = nn.Embedding(output_dim, emb_dim)\n        self.gru = nn.GRU(emb_dim + hidden_dim, hidden_dim)\n        self.fc_out = nn.Linear(hidden_dim * 2, output_dim)\n\n    def forward(self, input, hidden, encoder_outputs):\n        input = input.unsqueeze(0)  # [1, batch]\n        embedded = self.embedding(input)  # [1, batch, emb]\n\n        context,attn_weights = self.attention(hidden, encoder_outputs)  # [batch, hidden]\n        context = context.unsqueeze(0)  # [1, batch, hidden]\n\n        rnn_input = torch.cat((embedded, context), dim=2)  # [1, batch, emb + hidden]\n        output, hidden = self.gru(rnn_input, hidden)\n\n        output = output.squeeze(0)\n        context = context.squeeze(0)\n        prediction = self.fc_out(torch.cat((output, context), dim=1))  # [batch, output_dim]\n\n        return prediction, hidden, attn_weights\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-21T16:08:34.383613Z","iopub.execute_input":"2025-06-21T16:08:34.383904Z","iopub.status.idle":"2025-06-21T16:08:34.404886Z","shell.execute_reply.started":"2025-06-21T16:08:34.383874Z","shell.execute_reply":"2025-06-21T16:08:34.404121Z"}},"outputs":[],"execution_count":23},{"cell_type":"code","source":"class Seq2Seq(nn.Module):\n    def __init__(self, encoder, decoder):\n        super().__init__()\n        self.encoder = encoder\n        self.decoder = decoder\n\n    def forward(self, x, y, teacher_forcing_ratio=0.5):\n        batch_size = y.shape[1]\n        y_len = y.shape[0]\n        y_vocab_size = self.decoder.fc_out.out_features\n        outputs = torch.zeros(y_len, batch_size, y_vocab_size)\n\n        encoder_outputs, hidden = self.encoder(x)\n        input = y[0, :]  #token\n\n        for t in range(1, y_len):\n            output, hidden, attn_weights = self.decoder(input, hidden, encoder_outputs)\n            outputs[t] = output\n            top1 = output.argmax(1)\n            input = y[t] if torch.rand(1).item() < teacher_forcing_ratio else top1\n\n        return outputs","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-21T16:08:34.405829Z","iopub.execute_input":"2025-06-21T16:08:34.406140Z","iopub.status.idle":"2025-06-21T16:08:34.427196Z","shell.execute_reply.started":"2025-06-21T16:08:34.406117Z","shell.execute_reply":"2025-06-21T16:08:34.426264Z"}},"outputs":[],"execution_count":24},{"cell_type":"code","source":"hidden_dim=512\ninput_dim = len(hi_vocab)\noutput_dim = len(en_vocab)\nemb_dim = 256\n\nattention = LuongAttention(hidden_dim)\ndecoder = AttentionDecoderGRU(output_dim, emb_dim, hidden_dim, attention)\n\n\nmodel = Seq2Seq(encoder, decoder).to(device)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-21T16:08:34.430697Z","iopub.execute_input":"2025-06-21T16:08:34.430951Z","iopub.status.idle":"2025-06-21T16:08:37.915515Z","shell.execute_reply.started":"2025-06-21T16:08:34.430925Z","shell.execute_reply":"2025-06-21T16:08:37.914773Z"}},"outputs":[],"execution_count":25},{"cell_type":"code","source":"attention_weights = []\n\nmax_len = 100\nfor _ in range(max_len):\n    with torch.no_grad():\n        output, hidden, attn = decoder(input_token, hidden, encoder_outputs)\n\n    attention_weights.append(attn.squeeze(1).cpu().numpy()) \n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"optimizer = torch.optim.AdamW(model.parameters(), lr=0.001)\ncriterion = nn.CrossEntropyLoss(ignore_index=en_vocab[\"<pad>\"])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-21T16:08:37.916338Z","iopub.execute_input":"2025-06-21T16:08:37.916586Z","iopub.status.idle":"2025-06-21T16:08:41.294556Z","shell.execute_reply.started":"2025-06-21T16:08:37.916566Z","shell.execute_reply":"2025-06-21T16:08:41.293834Z"}},"outputs":[],"execution_count":26},{"cell_type":"code","source":"EPOCHS = 10\n\nfor epoch in range(EPOCHS):\n    model.train()\n    total_loss = 0\n    count = 0\n    for src_batch, trg_batch in loader :\n        if count == 2:\n            break\n        count +=1\n        src_batch, trg_batch = src_batch.to(device), trg_batch.to(device)\n        \n        optimizer.zero_grad()\n        output = model(src_batch, trg_batch)  # [trg_len, batch, vocab_size]\n\n        # Reshape for loss\n        output_dim = output.shape[-1]\n        output = output[1:].reshape(-1, output_dim)         # remove <sos>\n        trg = trg_batch[1:].reshape(-1)                     # align targets\n\n        loss = criterion(output, trg)\n        loss.backward()\n        optimizer.step()\n\n        total_loss += loss.item()\n        del trg,output,output_dim \n        gc.collect() \n    avg_loss = total_loss / len(loader)\n    print(f\"Epoch {epoch+1} | Loss: {avg_loss:.4f}\")\n","metadata":{"execution":{"iopub.status.busy":"2025-06-21T16:08:41.295289Z","iopub.execute_input":"2025-06-21T16:08:41.295671Z","iopub.status.idle":"2025-06-21T16:43:24.377905Z","shell.execute_reply.started":"2025-06-21T16:08:41.295650Z","shell.execute_reply":"2025-06-21T16:43:24.376951Z"},"trusted":true},"outputs":[{"name":"stdout","text":"Epoch 1 | Loss: 0.0003\nEpoch 2 | Loss: 0.0002\nEpoch 3 | Loss: 0.0002\nEpoch 4 | Loss: 0.0002\nEpoch 5 | Loss: 0.0002\nEpoch 6 | Loss: 0.0002\nEpoch 7 | Loss: 0.0002\nEpoch 8 | Loss: 0.0002\nEpoch 9 | Loss: 0.0002\nEpoch 10 | Loss: 0.0002\n","output_type":"stream"}],"execution_count":27},{"cell_type":"code","source":"del loader\ngc.collect()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-21T16:43:31.062883Z","iopub.execute_input":"2025-06-21T16:43:31.063668Z","iopub.status.idle":"2025-06-21T16:43:32.371523Z","shell.execute_reply.started":"2025-06-21T16:43:31.063641Z","shell.execute_reply":"2025-06-21T16:43:32.370572Z"}},"outputs":[{"execution_count":28,"output_type":"execute_result","data":{"text/plain":"51"},"metadata":{}}],"execution_count":28},{"cell_type":"code","source":"def translate(sentence, model, hi_vocab, en_vocab, en_ivocab, max_len=50): \n    model.eval() \n    tokens = word_tokenize(sentence) \n    indices = [hi_vocab[\"<sos>\"]] + [hi_vocab.get(tok, hi_vocab[\"<unk>\"]) for tok in tokens] + [hi_vocab[\"<eos>\"]] \n    src_tensor = torch.tensor(indices).unsqueeze(1).to(device) # [src_len, 1] \n    with torch.no_grad(): \n        encoder_outputs, hidden = model.encoder(src_tensor) \n    input_token = torch.tensor([en_vocab[\"<sos>\"]]).to(device) \n    output_tokens = [] \n    for _ in range(max_len): \n        with torch.no_grad(): \n            output, hidden, _ = model.decoder(input_token, hidden, encoder_outputs) \n        top1 = output.argmax(1).item() \n        if top1 == en_vocab[\"<eos>\"]:\n            break \n        output_tokens.append(top1) \n        input_token = torch.tensor([top1]).to(device) \n    return \" \".join([en_ivocab.get(tok, \"<unk>\") for tok in output_tokens])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-21T16:43:34.771812Z","iopub.execute_input":"2025-06-21T16:43:34.772143Z","iopub.status.idle":"2025-06-21T16:43:34.780969Z","shell.execute_reply.started":"2025-06-21T16:43:34.772111Z","shell.execute_reply":"2025-06-21T16:43:34.779946Z"}},"outputs":[],"execution_count":29},{"cell_type":"code","source":"translated = translate(\"मेरा नाम राम है\", model, hi_vocab, en_vocab, en_ivocab)\nprint(\"Predicted Translation:\", translated)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-21T16:43:39.551075Z","iopub.execute_input":"2025-06-21T16:43:39.551395Z","iopub.status.idle":"2025-06-21T16:43:39.609472Z","shell.execute_reply.started":"2025-06-21T16:43:39.551372Z","shell.execute_reply":"2025-06-21T16:43:39.608523Z"}},"outputs":[{"name":"stdout","text":"Predicted Translation: \n","output_type":"stream"}],"execution_count":30},{"cell_type":"code","source":"translate(\"मैं स्कूल जा रहा हूँ\", model, hi_vocab, en_vocab, en_ivocab)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-21T16:43:45.755770Z","iopub.execute_input":"2025-06-21T16:43:45.756146Z","iopub.status.idle":"2025-06-21T16:43:47.601188Z","shell.execute_reply.started":"2025-06-21T16:43:45.756119Z","shell.execute_reply":"2025-06-21T16:43:47.600460Z"}},"outputs":[{"execution_count":31,"output_type":"execute_result","data":{"text/plain":"', , , the of the of the of the of the of the of the of the of the of the of the of the of the of the of the of the of the of the .'"},"metadata":{}}],"execution_count":31},{"cell_type":"code","source":"translate(\"तुम बहुत सुंदर हो\", model, hi_vocab, en_vocab, en_ivocab)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-21T16:43:52.294648Z","iopub.execute_input":"2025-06-21T16:43:52.294972Z","iopub.status.idle":"2025-06-21T16:43:52.361714Z","shell.execute_reply.started":"2025-06-21T16:43:52.294948Z","shell.execute_reply":"2025-06-21T16:43:52.360929Z"}},"outputs":[{"execution_count":32,"output_type":"execute_result","data":{"text/plain":"''"},"metadata":{}}],"execution_count":32},{"cell_type":"code","source":"hypotheses = []\nreferences = []\ncount = 0\nfor src_seq, trg_seq in zip(test_src, test_trg):  # token ID lists\n    if count == 20:\n        break\n  \n    src_sentence = [hi_ivocab.get(tok, \"<unk>\") for tok in src_seq if tok not in [hi_vocab[\"<sos>\"], hi_vocab[\"<eos>\"], hi_vocab[\"<pad>\"]]]\n    ref_sentence = [en_ivocab.get(tok, \"<unk>\") for tok in trg_seq if tok not in [en_vocab[\"<sos>\"], en_vocab[\"<eos>\"], en_vocab[\"<pad>\"]]]\n   \n    src_text = \" \".join(src_sentence)\n    ref_text = \" \".join(ref_sentence)\n    \n    prediction = translate(src_text, model, hi_vocab, en_vocab, en_ivocab)  # your existing translate() function\n   \n    hypotheses.append(prediction)\n    references.append([ref_text])\n    count +=1","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-21T16:43:56.415254Z","iopub.execute_input":"2025-06-21T16:43:56.416080Z","iopub.status.idle":"2025-06-21T16:43:59.202587Z","shell.execute_reply.started":"2025-06-21T16:43:56.416049Z","shell.execute_reply":"2025-06-21T16:43:59.201778Z"}},"outputs":[],"execution_count":33},{"cell_type":"code","source":"from sacrebleu import corpus_bleu\nbleu = corpus_bleu(hypotheses, references)\nprint(f\"BLEU Score: {bleu.score:.2f}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-21T16:44:02.220556Z","iopub.execute_input":"2025-06-21T16:44:02.221240Z","iopub.status.idle":"2025-06-21T16:44:02.340634Z","shell.execute_reply.started":"2025-06-21T16:44:02.221207Z","shell.execute_reply":"2025-06-21T16:44:02.339689Z"}},"outputs":[{"name":"stdout","text":"BLEU Score: 0.00\n","output_type":"stream"}],"execution_count":34},{"cell_type":"code","source":"del model\ngc.collect()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-21T16:44:05.028084Z","iopub.execute_input":"2025-06-21T16:44:05.028565Z","iopub.status.idle":"2025-06-21T16:44:06.294279Z","shell.execute_reply.started":"2025-06-21T16:44:05.028541Z","shell.execute_reply":"2025-06-21T16:44:06.293428Z"}},"outputs":[{"execution_count":35,"output_type":"execute_result","data":{"text/plain":"0"},"metadata":{}}],"execution_count":35},{"cell_type":"code","source":"del translate,optimizer,criterion,decoder,encoder\ngc.collect()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-21T16:46:42.338893Z","iopub.execute_input":"2025-06-21T16:46:42.339237Z","iopub.status.idle":"2025-06-21T16:46:43.971344Z","shell.execute_reply.started":"2025-06-21T16:46:42.339210Z","shell.execute_reply":"2025-06-21T16:46:43.970321Z"}},"outputs":[{"execution_count":40,"output_type":"execute_result","data":{"text/plain":"438"},"metadata":{}}],"execution_count":40},{"cell_type":"markdown","source":"# Part 3","metadata":{}},{"cell_type":"code","source":"pip install transformers datasets sacrebleu","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-21T17:20:32.247858Z","iopub.execute_input":"2025-06-21T17:20:32.248301Z","iopub.status.idle":"2025-06-21T17:20:38.797886Z","shell.execute_reply.started":"2025-06-21T17:20:32.248275Z","shell.execute_reply":"2025-06-21T17:20:38.797146Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.51.3)\nRequirement already satisfied: datasets in /usr/local/lib/python3.11/dist-packages (3.6.0)\nCollecting sacrebleu\n  Downloading sacrebleu-2.5.1-py3-none-any.whl.metadata (51 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m51.8/51.8 kB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.18.0)\nRequirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.31.1)\nRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (25.0)\nRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\nRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\nRequirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\nRequirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.1)\nRequirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\nRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\nRequirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (19.0.1)\nRequirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.3.8)\nRequirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets) (2.2.3)\nRequirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets) (3.5.0)\nRequirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.70.16)\nCollecting fsspec<=2025.3.0,>=2023.1.0 (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets)\n  Downloading fsspec-2025.3.0-py3-none-any.whl.metadata (11 kB)\nCollecting portalocker (from sacrebleu)\n  Downloading portalocker-3.2.0-py3-none-any.whl.metadata (8.7 kB)\nRequirement already satisfied: tabulate>=0.8.9 in /usr/local/lib/python3.11/dist-packages (from sacrebleu) (0.9.0)\nRequirement already satisfied: colorama in /usr/local/lib/python3.11/dist-packages (from sacrebleu) (0.4.6)\nRequirement already satisfied: lxml in /usr/local/lib/python3.11/dist-packages (from sacrebleu) (5.3.1)\nRequirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (3.11.18)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (4.13.2)\nRequirement already satisfied: hf-xet<2.0.0,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (1.1.0)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (2025.1.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (2022.1.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (2.4.1)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.2)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.4.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.4.26)\nRequirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\nRequirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\nRequirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2.6.1)\nRequirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.3.2)\nRequirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (25.3.0)\nRequirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.6.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (6.4.3)\nRequirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (0.3.1)\nRequirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.20.0)\nRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->transformers) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->transformers) (2022.1.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy>=1.17->transformers) (1.3.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy>=1.17->transformers) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy>=1.17->transformers) (2024.2.0)\nDownloading sacrebleu-2.5.1-py3-none-any.whl (104 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m104.1/104.1 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading fsspec-2025.3.0-py3-none-any.whl (193 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m193.6/193.6 kB\u001b[0m \u001b[31m9.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading portalocker-3.2.0-py3-none-any.whl (22 kB)\nInstalling collected packages: portalocker, fsspec, sacrebleu\n  Attempting uninstall: fsspec\n    Found existing installation: fsspec 2025.3.2\n    Uninstalling fsspec-2025.3.2:\n      Successfully uninstalled fsspec-2025.3.2\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ncesium 0.12.4 requires numpy<3.0,>=2.0, but you have numpy 1.26.4 which is incompatible.\nbigframes 1.42.0 requires rich<14,>=12.4.4, but you have rich 14.0.0 which is incompatible.\ntorch 2.6.0+cu124 requires nvidia-cublas-cu12==12.4.5.8; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cublas-cu12 12.9.0.13 which is incompatible.\ntorch 2.6.0+cu124 requires nvidia-cudnn-cu12==9.1.0.70; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cudnn-cu12 9.3.0.75 which is incompatible.\ntorch 2.6.0+cu124 requires nvidia-cufft-cu12==11.2.1.3; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cufft-cu12 11.4.0.6 which is incompatible.\ntorch 2.6.0+cu124 requires nvidia-curand-cu12==10.3.5.147; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-curand-cu12 10.3.10.19 which is incompatible.\ntorch 2.6.0+cu124 requires nvidia-cusolver-cu12==11.6.1.9; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cusolver-cu12 11.7.4.40 which is incompatible.\ntorch 2.6.0+cu124 requires nvidia-cusparse-cu12==12.3.1.170; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cusparse-cu12 12.5.9.5 which is incompatible.\ntorch 2.6.0+cu124 requires nvidia-nvjitlink-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-nvjitlink-cu12 12.9.41 which is incompatible.\ngcsfs 2025.3.2 requires fsspec==2025.3.2, but you have fsspec 2025.3.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed fsspec-2025.3.0 portalocker-3.2.0 sacrebleu-2.5.1\nNote: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n\nmodel_name = \"Helsinki-NLP/opus-mt-hi-en\"\ntokenizer = AutoTokenizer.from_pretrained(model_name)\nmodel = AutoModelForSeq2SeqLM.from_pretrained(model_name)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-21T17:20:42.571398Z","iopub.execute_input":"2025-06-21T17:20:42.571695Z","iopub.status.idle":"2025-06-21T17:21:16.627038Z","shell.execute_reply.started":"2025-06-21T17:20:42.571666Z","shell.execute_reply":"2025-06-21T17:21:16.626309Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/42.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fb5db925f7aa4ff394ff87018eebe12f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/1.38k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c1e26693d0194bd9b78b95073edc0baa"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"source.spm:   0%|          | 0.00/1.06M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b547c12938b84beb89f64963833a8cbb"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"target.spm:   0%|          | 0.00/813k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d8209ca7fa3b4039bc54b5719197c3cb"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.json:   0%|          | 0.00/2.06M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"82a95505ea4d4c87bcc333adf3e78d45"}},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/transformers/models/marian/tokenization_marian.py:175: UserWarning: Recommended: pip install sacremoses.\n  warnings.warn(\"Recommended: pip install sacremoses.\")\n2025-06-21 17:20:58.454194: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1750526458.858293      35 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1750526458.966925      35 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"pytorch_model.bin:   0%|          | 0.00/304M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"93fa245f77c548049c852b8772ead814"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/293 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2a4c481e74a848998445f21b83b50db2"}},"metadata":{}}],"execution_count":4},{"cell_type":"code","source":"from datasets import Dataset, DatasetDict\n\n# Load CSV with pandas\ndf = pd.read_csv('/kaggle/input/english-to-hindi/hindi_english_parallel.csv')\ndf = df.dropna(subset=[\"hindi\", \"english\"])\n\n# Convert to Hugging Face Dataset\ndataset = Dataset.from_pandas(df)\n\n# Split into train and test sets\ndataset = dataset.train_test_split(test_size=0.1) \n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-21T17:21:16.628540Z","iopub.execute_input":"2025-06-21T17:21:16.628965Z","iopub.status.idle":"2025-06-21T17:21:33.503136Z","shell.execute_reply.started":"2025-06-21T17:21:16.628937Z","shell.execute_reply":"2025-06-21T17:21:33.502545Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/304M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2d8996377c534629834bc1d8aaf3cf38"}},"metadata":{}}],"execution_count":5},{"cell_type":"code","source":"import gc","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-21T17:21:45.623873Z","iopub.execute_input":"2025-06-21T17:21:45.624145Z","iopub.status.idle":"2025-06-21T17:21:45.627817Z","shell.execute_reply.started":"2025-06-21T17:21:45.624122Z","shell.execute_reply":"2025-06-21T17:21:45.627098Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"del df\ngc.collect()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"dataset = DatasetDict({\n    \"train\": dataset[\"train\"],\n    \"test\": dataset[\"test\"]\n})\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-21T17:22:07.894750Z","iopub.execute_input":"2025-06-21T17:22:07.895208Z","iopub.status.idle":"2025-06-21T17:22:07.898661Z","shell.execute_reply.started":"2025-06-21T17:22:07.895183Z","shell.execute_reply":"2025-06-21T17:22:07.898103Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"prefix = \"translate Hindi to English: \"\n\ndef preprocess(example):\n    inputs = [prefix + (text if text is not None else \"\") for text in example[\"hindi\"]]\n    targets = [(text if text is not None else \"\") for text in example[\"english\"]]\n\n    model_inputs = tokenizer(inputs, max_length=128, truncation=True, padding=\"max_length\")\n    with tokenizer.as_target_tokenizer():\n        labels = tokenizer(targets, max_length=128, truncation=True, padding=\"max_length\")\n\n    model_inputs[\"labels\"] = labels[\"input_ids\"]\n    return model_inputs\n\n\n\ntokenized = dataset.map(preprocess, batched=True)\nprint(dataset[\"train\"].column_names)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-21T17:22:10.438778Z","iopub.execute_input":"2025-06-21T17:22:10.439039Z","iopub.status.idle":"2025-06-21T17:31:09.990028Z","shell.execute_reply.started":"2025-06-21T17:22:10.439018Z","shell.execute_reply":"2025-06-21T17:31:09.989439Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/1400016 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b131853cfd4f4975a3a1405b32936f55"}},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/transformers/tokenization_utils_base.py:3980: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/155558 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1597b399f7a8485e86bbad09aef33887"}},"metadata":{}},{"name":"stdout","text":"['hindi', 'english', '__index_level_0__']\n","output_type":"stream"}],"execution_count":10},{"cell_type":"code","source":"import transformers\nprint(transformers.__version__)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-21T17:31:20.874324Z","iopub.execute_input":"2025-06-21T17:31:20.874580Z","iopub.status.idle":"2025-06-21T17:31:20.878841Z","shell.execute_reply.started":"2025-06-21T17:31:20.874563Z","shell.execute_reply":"2025-06-21T17:31:20.878200Z"}},"outputs":[{"name":"stdout","text":"4.51.3\n","output_type":"stream"}],"execution_count":11},{"cell_type":"code","source":"from transformers import Seq2SeqTrainer, Seq2SeqTrainingArguments\n\ntraining_args = Seq2SeqTrainingArguments(\n    output_dir=\"./mt-hi-en\",\n    learning_rate=2e-5,\n    per_device_train_batch_size=16,\n    per_device_eval_batch_size=16,\n    weight_decay=0.01,\n    save_total_limit=2,\n    num_train_epochs=3,\n    predict_with_generate=True,\n    fp16=True\n)\n\ntrainer = Seq2SeqTrainer(\n    model=model,\n    args=training_args,\n    train_dataset=tokenized[\"train\"],\n    eval_dataset=tokenized[\"test\"],\n    tokenizer=tokenizer\n)\n\ntrainer.train()\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def translate(text):\n    inputs = tokenizer(prefix + text, return_tensors=\"pt\", truncation=True)\n\n    # Ensure inputs go to the same device as the model\n    inputs = {key: value.to(model.device) for key, value in inputs.items()}\n\n    outputs = model.generate(**inputs, max_length=50)\n    return tokenizer.decode(outputs[0], skip_special_tokens=True)\n\nprint(translate(\"मेरा नाम राम है।\"))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-21T17:34:03.883145Z","iopub.execute_input":"2025-06-21T17:34:03.883923Z","iopub.status.idle":"2025-06-21T17:34:06.315636Z","shell.execute_reply.started":"2025-06-21T17:34:03.883897Z","shell.execute_reply":"2025-06-21T17:34:06.314782Z"}},"outputs":[{"name":"stdout","text":"The name is Hindi Toch: My name is Ram.\n","output_type":"stream"}],"execution_count":14},{"cell_type":"code","source":"pip install evaluate\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-21T17:35:45.106088Z","iopub.execute_input":"2025-06-21T17:35:45.106707Z","iopub.status.idle":"2025-06-21T17:35:48.893143Z","shell.execute_reply.started":"2025-06-21T17:35:45.106678Z","shell.execute_reply":"2025-06-21T17:35:48.892311Z"}},"outputs":[{"name":"stdout","text":"Collecting evaluate\n  Downloading evaluate-0.4.4-py3-none-any.whl.metadata (9.5 kB)\nRequirement already satisfied: datasets>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from evaluate) (3.6.0)\nRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from evaluate) (1.26.4)\nRequirement already satisfied: dill in /usr/local/lib/python3.11/dist-packages (from evaluate) (0.3.8)\nRequirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from evaluate) (2.2.3)\nRequirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.11/dist-packages (from evaluate) (2.32.3)\nRequirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.11/dist-packages (from evaluate) (4.67.1)\nRequirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from evaluate) (3.5.0)\nRequirement already satisfied: multiprocess in /usr/local/lib/python3.11/dist-packages (from evaluate) (0.70.16)\nRequirement already satisfied: fsspec>=2021.05.0 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]>=2021.05.0->evaluate) (2025.3.0)\nRequirement already satisfied: huggingface-hub>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from evaluate) (0.31.1)\nRequirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from evaluate) (25.0)\nRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from datasets>=2.0.0->evaluate) (3.18.0)\nRequirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets>=2.0.0->evaluate) (19.0.1)\nRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from datasets>=2.0.0->evaluate) (6.0.2)\nRequirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]>=2021.05.0->evaluate) (3.11.18)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.7.0->evaluate) (4.13.2)\nRequirement already satisfied: hf-xet<2.0.0,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.7.0->evaluate) (1.1.0)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->evaluate) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->evaluate) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->evaluate) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->evaluate) (2025.1.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->evaluate) (2022.1.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->evaluate) (2.4.1)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->evaluate) (3.4.2)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->evaluate) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->evaluate) (2.4.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->evaluate) (2025.4.26)\nRequirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->evaluate) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->evaluate) (2025.2)\nRequirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->evaluate) (2025.2)\nRequirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (2.6.1)\nRequirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (1.3.2)\nRequirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (25.3.0)\nRequirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (1.6.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (6.4.3)\nRequirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (0.3.1)\nRequirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (1.20.0)\nRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->evaluate) (1.17.0)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->evaluate) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->evaluate) (2022.1.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy>=1.17->evaluate) (1.3.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy>=1.17->evaluate) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy>=1.17->evaluate) (2024.2.0)\nDownloading evaluate-0.4.4-py3-none-any.whl (84 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.1/84.1 kB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hInstalling collected packages: evaluate\nSuccessfully installed evaluate-0.4.4\nNote: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}],"execution_count":16},{"cell_type":"code","source":"import evaluate\n\nmetric = evaluate.load(\"sacrebleu\")\n\ndef compute_metrics(eval_pred):\n    predictions, labels = eval_pred\n    decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)\n    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n    return metric.compute(predictions=decoded_preds, references=[[l] for l in decoded_labels])\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-21T17:36:03.697350Z","iopub.execute_input":"2025-06-21T17:36:03.697673Z","iopub.status.idle":"2025-06-21T17:36:04.739776Z","shell.execute_reply.started":"2025-06-21T17:36:03.697644Z","shell.execute_reply":"2025-06-21T17:36:04.738977Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading builder script:   0%|          | 0.00/8.15k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1748d46772234c8e975f4bf02607760e"}},"metadata":{}}],"execution_count":17},{"cell_type":"code","source":"import torch\nfrom tqdm import tqdm\n\nmodel.eval()\n\npredictions = []\nreferences = []\nsources = []\n\nfor example in tqdm(tokenized[\"test\"]):\n    # Get input and target text from original columns\n    src_text = example[\"hindi\"]\n    ref_text = example[\"english\"]\n\n    # Encode input\n    inputs = tokenizer(\"translate Hindi to English: \" + src_text, return_tensors=\"pt\", truncation=True).to(model.device)\n\n    # Generate prediction\n    with torch.no_grad():\n        output_tokens = model.generate(**inputs, max_length=50)\n    \n    pred_text = tokenizer.decode(output_tokens[0], skip_special_tokens=True)\n\n    # Store everything\n    predictions.append(pred_text)\n    references.append([ref_text])  # wrap in list for sacrebleu format\n    sources.append(src_text)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import evaluate\n\nbleu = evaluate.load(\"sacrebleu\")\nresult = bleu.compute(predictions=predictions, references=references)\n\nprint(f\"BLEU score: {result['score']:.2f}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-21T17:51:54.632212Z","iopub.execute_input":"2025-06-21T17:51:54.632646Z","iopub.status.idle":"2025-06-21T17:51:55.097673Z","shell.execute_reply.started":"2025-06-21T17:51:54.632621Z","shell.execute_reply":"2025-06-21T17:51:55.097064Z"}},"outputs":[{"name":"stdout","text":"BLEU score: 14.58\n","output_type":"stream"}],"execution_count":26}]}